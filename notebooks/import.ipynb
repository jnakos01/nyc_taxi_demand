{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81447f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports all data required for the project.\n",
    "Processes and saves parquets.\n",
    "Requires Subway Data CSV and NY Boroughs folder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe71d9",
   "metadata": {},
   "source": [
    "# Data Import (Landing Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d8276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import requests\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074294da",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = [\"taxi_data\", \"weather_data\", \"event_data\", \"subway_data\"]\n",
    "\n",
    "# Create a folder to store the data\n",
    "data_folder = \"../data/landing\"\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "# Create the path for each type of dataset\n",
    "output_dirs = []\n",
    "# TLC (Yellow taxi), Weather, and Event Data\n",
    "for target_dir in folder_names:\n",
    "    dir_path = os.path.join(data_folder, target_dir)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    output_dirs.append(dir_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914c7e3",
   "metadata": {},
   "source": [
    "## Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdec4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Taxi data for 2024 completed.\n"
     ]
    }
   ],
   "source": [
    "# Define our range of interest\n",
    "YEARS = [2024]\n",
    "MONTHS = range(1, 13)\n",
    "\n",
    "# Base URL for Yellow Taxi data\n",
    "BASE_URL = BASE = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_\"\n",
    "\n",
    "# Download Yellow Taxi data\n",
    "for year in YEARS:\n",
    "    for month in MONTHS:\n",
    "        fname = f\"{year}-{month:02d}.parquet\"\n",
    "        url = BASE_URL + fname\n",
    "        output = os.path.join(output_dirs[0], fname)\n",
    "        if not os.path.exists(output):\n",
    "            print(f\"Downloading {fname} to {output}\")\n",
    "            urllib.request.urlretrieve(url, output)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(f\"Yellow Taxi data for {year} completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207de520",
   "metadata": {},
   "source": [
    "#### Get borough coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74066b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        BoroName        lat        lon\n",
      "0  Staten Island  40.580833 -74.153405\n",
      "1       Brooklyn  40.644746 -73.947855\n",
      "2         Queens  40.707626 -73.818545\n",
      "3      Manhattan  40.777239 -73.967200\n",
      "4          Bronx  40.852635 -73.866508\n",
      "5            EWR  40.689491 -74.174538\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(\"../data/ny_boroughs/nybb.shp\")\n",
    "\n",
    "# Project to a different coord reference system for better accuracy\n",
    "gdf_proj = gdf.to_crs(epsg=2263)\n",
    "\n",
    "# Compute controids to pull weather from\n",
    "gdf_proj[\"centroid\"] = gdf_proj.geometry.centroid\n",
    "\n",
    "# Convert back to lat/lon (WGS84) for export\n",
    "centroids_latlon = gdf_proj.set_geometry(\"centroid\").to_crs(epsg=4326)\n",
    "\n",
    "# Get lat long of centroids\n",
    "centroids_latlon[\"lat\"] = centroids_latlon.geometry.y\n",
    "centroids_latlon[\"lon\"] = centroids_latlon.geometry.x\n",
    "\n",
    "\n",
    "centroids = centroids_latlon[[\"BoroName\", \"lat\", \"lon\"]].copy()\n",
    "\n",
    "# Add newark airport coords\n",
    "# From https://www.latlong.net/place/newark-liberty-international-airport-nj-usa-33210.html\n",
    "\n",
    "# There are very few centroids so I will use pandas for this task\n",
    "\n",
    "ewr_row = pd.DataFrame([{\"BoroName\": \"EWR\", \"lat\": 40.689491, \"lon\": -74.174538}])\n",
    "centroids = pd.concat([centroids, ewr_row], ignore_index=True)\n",
    "\n",
    "print(centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfffbf5",
   "metadata": {},
   "source": [
    "#### Import Taxi Zone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f70448c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "      <th>centroid</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POINT (-74.17400009529821 40.691831156065916)</td>\n",
       "      <td>40.691831</td>\n",
       "      <td>-74.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>POINT (-73.83129879600592 40.61674466473727)</td>\n",
       "      <td>40.616745</td>\n",
       "      <td>-73.831299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>POINT (-73.84742222112834 40.8644731447022)</td>\n",
       "      <td>40.864473</td>\n",
       "      <td>-73.847422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID Borough                     Zone service_zone  \\\n",
       "0           1     EWR           Newark Airport          EWR   \n",
       "1           2  Queens              Jamaica Bay    Boro Zone   \n",
       "2           3   Bronx  Allerton/Pelham Gardens    Boro Zone   \n",
       "\n",
       "                                        centroid  centroid_lat  centroid_lon  \n",
       "0  POINT (-74.17400009529821 40.691831156065916)     40.691831    -74.174000  \n",
       "1   POINT (-73.83129879600592 40.61674466473727)     40.616745    -73.831299  \n",
       "2    POINT (-73.84742222112834 40.8644731447022)     40.864473    -73.847422  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import taxi zone Shapefiles and find the centroid of each zone\n",
    "\n",
    "# Add each centroid as a field in the lookup table\n",
    "\n",
    "zone_lookup = pd.read_csv(\"../data/import_csvs/taxi_zones/taxi_zone_lookup.csv\")\n",
    "\n",
    "# Read shapefile\n",
    "shapefile_path = \"../data/import_csvs/taxi_zones/shapefiles/taxi_zones.shp\"\n",
    "\n",
    "\n",
    "def find_write_centroids(zone_lookup_df, shapefile_path):\n",
    "    \"\"\"\n",
    "    Find the centroids of the taxi zones and write them to the lookup table.\n",
    "    Adds both the centroid geometry and its coordinates (lat/lon).\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    # Convert to WGS84 for lat/lon\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    for idx, row in gdf.iterrows():\n",
    "        centroid = row[\"geometry\"].centroid\n",
    "        zone_lookup_df.loc[\n",
    "            zone_lookup_df[\"LocationID\"] == row[\"LocationID\"], \"centroid\"\n",
    "        ] = centroid\n",
    "        zone_lookup_df.loc[\n",
    "            zone_lookup_df[\"LocationID\"] == row[\"LocationID\"], \"centroid_lat\"\n",
    "        ] = centroid.y\n",
    "        zone_lookup_df.loc[\n",
    "            zone_lookup_df[\"LocationID\"] == row[\"LocationID\"], \"centroid_lon\"\n",
    "        ] = centroid.x\n",
    "    zone_lookup_df.to_csv(\n",
    "        \"../data/import_csvs/taxi_zones/taxi_zone_lookup.csv\", index=False, header=True\n",
    "    )\n",
    "    return zone_lookup_df.head(3)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "find_write_centroids(zone_lookup, shapefile_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a49a29",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aae1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data Import\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"America/New_York\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes we want\n",
    "WEATHER_LIST = [\n",
    "    \"temperature_2m\",\n",
    "    \"snow_depth\",\n",
    "    \"snowfall\",\n",
    "    \"rain\",\n",
    "    \"precipitation\",\n",
    "    \"relative_humidity_2m\",\n",
    "    \"weather_code\",\n",
    "    \"cloud_cover\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_gusts_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"shortwave_radiation\",\n",
    "    \"apparent_temperature\",\n",
    "    \"surface_pressure\",\n",
    "    \"is_day\",\n",
    "    \"sunshine_duration\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_weather_data(lat, lon, start=\"2024-01-01\", end=\"2024-12-31\"):\n",
    "    \"\"\"\n",
    "    Pulls hourly weather data for given borough location from Open Meteo API.\n",
    "    Returns a dataframe with timestamps and weather attributes\n",
    "\n",
    "    \"\"\"\n",
    "    weather_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start,\n",
    "        \"end_date\": end,\n",
    "        \"hourly\": \",\".join(WEATHER_LIST),\n",
    "        \"timezone\": \"America/New_York\",\n",
    "    }\n",
    "\n",
    "    # Make API request\n",
    "    response = requests.get(weather_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract timestamps\n",
    "    timestamps = data[\"hourly\"][\"time\"]\n",
    "    n = len(timestamps)\n",
    "\n",
    "    # Build list of dicts directly from JSON\n",
    "    records = []\n",
    "    for i in range(n):\n",
    "        record = {\"timestamp\": timestamps[i]}\n",
    "        for attribute in WEATHER_LIST:\n",
    "            record[attribute] = data[\"hourly\"].get(attribute, [None] * n)[i]\n",
    "        records.append(record)\n",
    "\n",
    "    sdf = spark.createDataFrame(records)\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d0cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows read in 52704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Start with an empty Spark DataFrame\n",
    "weather_sdf = None\n",
    "\n",
    "# Fetch weather data for each borough\n",
    "for idx, borough in centroids.iterrows():\n",
    "    success = False\n",
    "    attempts = 0\n",
    "\n",
    "    while not success:\n",
    "        try:\n",
    "            curr_sdf = get_weather_data(borough[\"lat\"], borough[\"lon\"]).withColumn(\n",
    "                \"BoroName\", lit(borough[\"BoroName\"])\n",
    "            )\n",
    "\n",
    "            # Union (combine) new sdf into main sdf\n",
    "            if weather_sdf is None:\n",
    "                weather_sdf = curr_sdf\n",
    "            else:\n",
    "                weather_sdf = weather_sdf.unionByName(curr_sdf)\n",
    "\n",
    "            success = True\n",
    "\n",
    "        # Avoid being rate limited by sleeping\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                attempts += 1\n",
    "                wait_time = min(10 * attempts, 60)\n",
    "                print(f\"Hit rate limit, sleeping for {wait_time} seconds\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"HTTP error for borough\")\n",
    "                break\n",
    "        # Non HTTP related error\n",
    "        except Exception as e:\n",
    "            print(\"Other error has occurred\")\n",
    "            break\n",
    "\n",
    "# Save to Parquet after we read in data\n",
    "weather_dir = os.path.join(data_folder, \"weather_data\")\n",
    "\n",
    "print(f\"Number of rows read in {weather_sdf.count()}\")\n",
    "\n",
    "if weather_sdf is not None:\n",
    "    weather_sdf.write.mode(\"overwrite\").parquet(weather_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e23c7",
   "metadata": {},
   "source": [
    "## Subway (Metro) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9819cfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:======================================================> (29 + 1) / 30]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records read in: 27023937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "SUB_PARQUET_DIR = \"../data/landing/subway_data\"\n",
    "\n",
    "# Create directory if it doesn't exist (defensive as we should have already created it)\n",
    "os.makedirs(SUB_PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "num_rows = 0\n",
    "\n",
    "# Read only 2024's csv into landing/subway_data\n",
    "for year in [2024]:\n",
    "    sub_sdf = spark.read.csv(\n",
    "        f\"../data/import_csvs/subway_data/MTA_{year}.csv\", header=True, inferSchema=True\n",
    "    )\n",
    "    num_rows += sub_sdf.count()\n",
    "    sub_sdf.write.parquet(f\"{SUB_PARQUET_DIR}/{year}\", mode=\"overwrite\")\n",
    "\n",
    "print(f\"Records read in: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3885435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27023937\n"
     ]
    }
   ],
   "source": [
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71807506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
